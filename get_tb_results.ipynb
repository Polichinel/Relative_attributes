{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Så det er vel et empirisk spørgsmål om (0,0) skal væk. Men i rapporten tror jeg det gav meget bedre resultater. MEN hvis du fjerner dem ka ndu ikke mere sammenligne billeder på tværs af kodere....\n",
    "\n",
    "- Tjek forskel på hvor mange (0,0)' er hver kodere har\n",
    "- Tjek forskel på om du beholder (0,0) eller ej ift. meningsfulde resultater.\n",
    "- Tjek om du bliver nød til\n",
    "- Du har tre forskellige mål. Du skal se hvilke der giver mest meningsfulde resultater\n",
    "- Test om en ensample er bedere og hvor meget de tre resultater corrolere.\n",
    "\n",
    "**so now**:\n",
    "Make a for loop going over all features for all coders (right now just 1 and 3) and save this jazz in some nice format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can run from base\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# from functions import * # import util functions\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import choix\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df(att_dict = 1):\n",
    "    \"\"\"att_dict should be 1,2,3\"\"\"\n",
    "\n",
    "    path = \"/home/simon/Documents/Bodies/data/RA/att_dicts\"\n",
    "\n",
    "    file_name1 = \"pregenerated_indx_list.pkl\"\n",
    "    file_path = os.path.join(path, file_name1)\n",
    "\n",
    "    # Open the file in binary mode\n",
    "    with open(file_path, 'rb') as file:\n",
    "        \n",
    "        # Call load method to deserialze\n",
    "        pregenerated_indx_list = pickle.load(file)\n",
    "\n",
    "    file_name2 = f\"att_dict_{att_dict}.pkl\"\n",
    "    file_path = os.path.join(path, file_name2)\n",
    "\n",
    "    if os.path.exists(file_path):\n",
    "\n",
    "        # Open the file in binary mode\n",
    "        with open(file_path, 'rb') as file:\n",
    "        \n",
    "            # Call load method to deserialze\n",
    "            att_dict = pickle.load(file)\n",
    "\n",
    "        df_img = pd.DataFrame(pregenerated_indx_list, columns=['img1', 'img2'])\n",
    "        df_att = pd.DataFrame(att_dict, columns= att_dict.keys())\n",
    "        df_att.drop(['indx_indicator'], axis=1, inplace= True)\n",
    "        df = df_att.join(df_img)\n",
    "\n",
    "        columns_dict= { 'att0' : 'negative_emotions_t1', 'att1': 'negative_emotions_t2', 'att2': 'mass_protest', \n",
    "                        'att3': 'damaged_property', 'att4': 'privat', 'att5': 'public', \n",
    "                        'att6': 'militarized', 'att7': 'rural', 'att8': 'urban', 'att9': 'formal' }\n",
    "\n",
    "        df.rename(columns= columns_dict, inplace= True)\n",
    "\n",
    "    else: \n",
    "        print('That att_dict does not exist. Use 1, 2, or 3.')\n",
    "\n",
    "    return(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_zero_ratio(df):\n",
    "\n",
    "    print(\"Ratio of (0,0)'s in each feature:\\n\")\n",
    "\n",
    "    for i in df.columns[:-2]:\n",
    "        ratio = (df[i] == (0,0)).sum() / df.shape[0]\n",
    "        print(f'{i}: {ratio*100:.3}%')\n",
    "\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_network(G, plot = True):\n",
    "\n",
    "    print(f'Number of edges: {len(G.edges)}') # same as len(indx_list)\n",
    "    print(f'Number of nodes: {len(G.nodes)}') # same as len(indx_list)\n",
    "    print(f'Connected network: {nx.is_connected(G)}')\n",
    "\n",
    "    G_degrees = list(dict(G.degree).values())\n",
    "\n",
    "    print(f'Mean degrees: {np.mean(G_degrees)}')\n",
    "    print(f'Min degrees: {np.min(G_degrees)}')\n",
    "    print(f'Max degrees: {np.max(G_degrees)}')\n",
    "\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    if plot == True:\n",
    "\n",
    "        plt.figure(figsize=[20,5])\n",
    "\n",
    "        plt.subplot(1,3,1)\n",
    "        plt.title('network random layout')\n",
    "        nx.draw(G, pos=nx.random_layout(G), node_size = 50, node_color = 'blue', alpha = 0.2, width = 0.5, edge_color = 'black')\n",
    "\n",
    "        plt.subplot(1,3,2)\n",
    "        plt.title('network spring layout')\n",
    "        nx.draw(G, pos=nx.spring_layout(G), node_size = 50, node_color = 'blue', alpha = 0.2, width = 0.5, edge_color = 'black')\n",
    "\n",
    "        plt.subplot(1,3,3)\n",
    "        plt.title('network degree distribution')\n",
    "        plt.hist(G_degrees, density=True, bins= 50)\n",
    "        sns.kdeplot(G_degrees)\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_non_draw_connected_sub_df(df, att):\n",
    "\n",
    "    # remove draws\n",
    "    non_draw_sub = df[(df[att] != (0,0)) & (df[att] != (1,1))][[att, 'img1', 'img2']]\n",
    "\n",
    "    # get edge list from non-draw subset\n",
    "    edge_list_non_zero = list(zip(non_draw_sub['img1'], non_draw_sub['img2']))\n",
    "\n",
    "    # Full graph g\n",
    "    g = nx.Graph()\n",
    "    g.add_edges_from(edge_list_non_zero)\n",
    "\n",
    "    # Get larges connected subset:\n",
    "    connected_img = sorted(nx.connected_components(g), key = len, reverse=True)[0] # take the larges connected component - really the list you need.\n",
    "\n",
    "    edge_list_connected = [(node1, node2) for node1, node2 in edge_list_non_zero if node1 in connected_img or node2 in connected_img]\n",
    "\n",
    "\n",
    "    # Larges connected subgraph gc - just to check\n",
    "    gc = nx.Graph()\n",
    "    gc.add_edges_from(edge_list_connected)\n",
    "    analyse_network(gc, False)\n",
    "    \n",
    "\n",
    "    # sub df\n",
    "\n",
    "    non_draw_connected_sub_df = non_draw_sub[(non_draw_sub['img1'].isin(connected_img)) | (non_draw_sub['img2'].isin(connected_img))]\n",
    "\n",
    "    return(non_draw_connected_sub_df) \n",
    "\n",
    "#edge_list_non_zero = list(zip(att_sub_df['img1'], att_sub_df['img2']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_data(df, att):\n",
    "\n",
    "    non_draw_connected_sub_df = get_non_draw_connected_sub_df(df, att)\n",
    "\n",
    "    img_list = list(set(list(non_draw_connected_sub_df['img1']) +  list(non_draw_connected_sub_df['img2'])))\n",
    "    n_imgs = len(img_list)\n",
    "    img_idx_generator = img_list.index\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for i in range(non_draw_connected_sub_df.shape[0]):\n",
    "\n",
    "        img1_name = non_draw_connected_sub_df['img1'].iloc[i]\n",
    "        img2_name = non_draw_connected_sub_df['img2'].iloc[i]\n",
    "\n",
    "        img1_idx = img_idx_generator(img1_name)\n",
    "        img2_idx = img_idx_generator(img2_name)\n",
    "\n",
    "        \n",
    "        if non_draw_connected_sub_df[att].iloc[i][0] > non_draw_connected_sub_df[att].iloc[i][1]:\n",
    "            directed_edge = (img1_idx, img2_idx)\n",
    "            data.append(directed_edge)\n",
    "\n",
    "        elif non_draw_connected_sub_df[att].iloc[i][0] < non_draw_connected_sub_df[att].iloc[i][1]:\n",
    "            directed_edge = (img2_idx, img1_idx)\n",
    "            data.append(directed_edge)\n",
    "\n",
    "        else: \n",
    "            print(f'something wrong w/ edge')\n",
    "            pass\n",
    "\n",
    "    return(data, n_imgs, img_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_att_data_dict(df):\n",
    "\n",
    "    att_data_dict = {}\n",
    "\n",
    "    for att in df.columns[:-2]:\n",
    "\n",
    "        print(att)\n",
    "        \n",
    "        data, n_imgs, img_list = get_input_data(df, att)\n",
    "\n",
    "        att_data_dict[f'{att}_data'] = data\n",
    "        att_data_dict[f'{att}_n'] = n_imgs\n",
    "        att_data_dict[f'{att}_img_list'] = img_list\n",
    "\n",
    "\n",
    "    return(att_data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(df):\n",
    "\n",
    "    att_data_dict = get_att_data_dict(df1)\n",
    "    clear_output(wait=True)\n",
    "    \n",
    "    dict_of_dfs = {}\n",
    "\n",
    "    for att in df.columns[:-2]:\n",
    "\n",
    "        data = att_data_dict[f'{att}_data']\n",
    "        n_imgs = att_data_dict[f'{att}_n']\n",
    "\n",
    "        lsr_mean = choix.ilsr_pairwise(n_imgs, data, alpha=0.01) # 10-20 sec\n",
    "        print(f'{att} lsr done')\n",
    "        mm_mean = choix.mm_pairwise(n_imgs, data, alpha=0.05) # 2 ish min, needs a bit more reg/aalpha to converge\n",
    "        print(f'{att} mm done')\n",
    "        # eb_mean, eb_cov = choix.ep_pairwise(n_imgs, data, alpha=0.01, model = 'logit') # 20 ish min\n",
    "\n",
    "        #result_dict = {'lsr_mean' : lsr_mean,'img' : att_data_dict[f'{att}_img_list']}\n",
    "\n",
    "        result_dict = {'lsr_mean' : lsr_mean, 'mm_mean' : mm_mean , 'img' : att_data_dict[f'{att}_img_list']}\n",
    "        results_df = pd.DataFrame(result_dict)\n",
    "\n",
    "        dict_of_dfs[att] = results_df\n",
    "\n",
    "    return(dict_of_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative_emotions_t1 lsr done\n",
      "negative_emotions_t1 mm done\n",
      "negative_emotions_t2 lsr done\n",
      "negative_emotions_t2 mm done\n",
      "mass_protest lsr done\n",
      "mass_protest mm done\n",
      "damaged_property lsr done\n",
      "damaged_property mm done\n",
      "privat lsr done\n",
      "privat mm done\n",
      "public lsr done\n",
      "public mm done\n",
      "militarized lsr done\n",
      "militarized mm done\n",
      "rural lsr done\n",
      "rural mm done\n",
      "urban lsr done\n",
      "urban mm done\n",
      "formal lsr done\n",
      "formal mm done\n"
     ]
    }
   ],
   "source": [
    "df1 = get_df(1)\n",
    "print_zero_ratio(df1)\n",
    "dict_of_dfs1 = get_results(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name1 = 'dict_of_dfs1.pkl'\n",
    "\n",
    "with open(file_name1, 'wb') as file:\n",
    "      \n",
    "    # A new file will be created\n",
    "    pickle.dump(dict_of_dfs1, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative_emotions_t1 lsr done\n",
      "negative_emotions_t1 mm done\n",
      "negative_emotions_t2 lsr done\n",
      "negative_emotions_t2 mm done\n",
      "mass_protest lsr done\n",
      "mass_protest mm done\n",
      "damaged_property lsr done\n",
      "damaged_property mm done\n",
      "privat lsr done\n",
      "privat mm done\n",
      "public lsr done\n",
      "public mm done\n",
      "militarized lsr done\n",
      "militarized mm done\n",
      "rural lsr done\n",
      "rural mm done\n",
      "urban lsr done\n",
      "urban mm done\n",
      "formal lsr done\n",
      "formal mm done\n"
     ]
    }
   ],
   "source": [
    "df3 = get_df(3)\n",
    "print_zero_ratio(df3)\n",
    "dict_of_dfs3 = get_results(df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name3 = 'dict_of_dfs3.pkl'\n",
    "\n",
    "with open(file_name3, 'wb') as file:\n",
    "      \n",
    "    # A new file will be created\n",
    "    pickle.dump(dict_of_dfs3, file)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7fb89c559b55f63718050ee465cdf227a9a9d85e12316ba886c7a20b86ce8ca5"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
